# Vision Arena 프로젝트 분석 (문제/해결/가치/타깃/가격)

**대상 프로젝트**: Vision Arena (VQGAN 등 Vision Model 성능 비교·평가·관리 플랫폼)  
**작성 목적**: 제품/사업 관점에서 “왜 이게 필요하고, 누가 돈을 낼지”를 빠르게 합의하기 위한 정리  
**전제(현재 상태)**: 프론트엔드 중심(Next.js/React), 모델 데이터는 로컬 스토리지 기반, 시각화(리더보드·차트)와 관리자 화면(기본 골격) 존재

---

## 1. 사람들이 가진 문제는 무엇인가요?

### 사람들이 겪는 문제(한 문장)
**“모델 성능을 비교·검증하는 과정이 파편화되어 반복적으로 시간이 낭비되고, 의사결정이 감(感)과 문서/스프레드시트에 의존하게 된다.”**

### 누가 겪는 문제인가요? (직군, 상황, 빈도)
- **연구자(Researcher)**
  - **상황**: 논문/리포트 작성, 실험 결과 정리, 모델 간 비교 근거가 필요할 때
  - **빈도**: 실험/학습 반복 주기마다(주 단위~일 단위) 지속 발생
- **ML/비전 엔지니어(ML Engineer / Vision Engineer)**
  - **상황**: 제품 적용 모델 선정(성능·속도·비용 Trade-off), 릴리즈 전 회귀(regression) 체크
  - **빈도**: 신규 모델 후보가 생길 때마다 + 릴리즈/배포 주기마다 반복
- **리드/PM/의사결정자(Tech Lead / PM)**
  - **상황**: “어떤 모델을 선택할지”를 빠르게 결론 내리고 팀을 설득해야 할 때
  - **빈도**: 프로젝트 마일스톤/의사결정 시점마다 반복
- **운영/관리자(Admin)**
  - **상황**: 모델 등록 승인, 정책/가중치 변경, 로그 확인 등 운영 업무
  - **빈도**: 팀 규모가 커질수록 상시성 증가

### 이 문제는 언제 가장 불편해지나요?
- **모델이 5개 → 20개 → 100개로 늘어날 때** (비교 조합 수가 폭발)
- **지표가 늘어날 때** (PSNR/SSIM/LPIPS/FID + bpp + 시간 + FPS + Perplexity 등)
- **의사결정 압박이 큰 순간**
  - 데드라인 직전, 릴리즈 전날, 논문 제출 전, 데모/고객 미팅 전
- **팀이 커지고 역할이 분리될 때**
  - 실험자/평가자/결정자가 다르면 “같은 데이터를 같은 기준으로” 보기가 어려움

### 지금은 어떻게 해결하고 있나요? (불편한 대안)
- **스프레드시트(Excel/Google Sheets)**
  - 지표 입력/복사/정렬은 가능하지만, 다차원 비교(레이더/분포/상관)를 매번 손으로 만듦
- **실험 결과 폴더/로그를 직접 탐색(로컬/서버)**
  - 결과가 흩어지고, 같은 기준으로 비교하기 어렵고, “누가/언제/왜”가 남지 않음
- **논문/블로그/슬랙 메시지 기반의 근거 축적**
  - 정보는 쌓이지만 구조화되지 않아, 다음 의사결정 때 다시 처음부터 찾게 됨
- **개별 스크립트/노트북(Jupyter/Colab)로 시각화**
  - 한 번 만들면 좋지만 팀 전체 표준으로 운영하기 어렵고 재현성/접근성이 떨어짐

> **핵심 포인트**  
> - 문제는 “크기”보다 **반복성(repetition)**이 중요합니다.  
> - **자주 발생하는 작은 불편**(지표 모으기, 비교표 만들기, 근거 설명하기)이 누적되며 사업 기회가 됩니다.

---

## 2. 왜 지금까지 제대로 해결되지 않았나요?

### 기존 해결 방식의 한계는 무엇인가요?
- **표준화 부재**: 팀마다 지표 정의/가중치/측정 방식이 달라 결과 비교가 어려움
- **도구 파편화**: 실험은 A, 기록은 B, 시각화는 C… “한 곳에서 끝나지 않음”
- **비교의 다차원성**: 품질·압축·속도는 서로 상충(Trade-off)하며 단일 숫자로 끝나지 않음
- **운영 관점 부재**: 개인 노트북/스프레드시트는 ‘팀 운영(승인/로그/권한)’에 취약

### 비용, 복잡도, 시간, 기술 중 어떤 이유인가요?
- **시간(Time)**: 빠르게 실험을 돌려야 해서 제품화(툴링)에 투자하기 어려움
- **복잡도(Complexity)**: 지표/데이터/워크플로우가 다양해서 범용 솔루션이 어려움
- **비용(Cost)**: DB/권한/운영까지 갖춘 시스템은 초기 구축·유지 비용이 큼
- **기술(Tech)**: 시각화·데이터 파이프라인·재현성·권한 모델이 결합되어 난이도가 높음

---

## 3. 어떻게 해결하고 싶은가요?

### 핵심 해결 방식은 무엇인가요? (한 문장)
**“모델 성능 데이터를 한 곳에 모아 표준화된 지표로 자동 시각화/비교하고, 모델 선택 의사결정을 빠르게 만든다.”**

### 사용자는 무엇만 하면 되나요? (행동 단순화)
- 사용자는 **모델을 등록**하거나(또는 평가 결과를 가져오고)
- **비교하고 싶은 모델만 선택**하면
- 나머지는 시스템이 **자동으로**
  - 표준 지표로 정리
  - 레이더/분포/상관/비교 매트릭스로 시각화
  - 의사결정에 필요한 근거를 즉시 제공

### 기술인가요, 구조인가요, 경험(UX)인가요?
- **구조(Structure)**: 지표/모델/런(run)/로그를 구조화하고 비교 가능한 형태로 유지
- **경험(UX)**: “비교/설명/결정” 흐름을 한 화면에서 끝내도록 단순화
- **기술(Tech)**: (향후) 평가 파이프라인 연동, 데이터 수집 자동화, 권한/승인/감사 로그

---

## 4. 사람들이 느끼는 가치는 무엇인가요?

### 기능이 아니라 감정 변화(Emotion) 기준
- **결정 부담 감소**: “이 모델이 맞나?”라는 불안이 줄고, 근거가 명확해짐
- **스트레스 감소**: 데드라인 직전 ‘표 만들기/근거 찾기’ 반복이 사라짐
- **통제감(Control) 증가**: 팀이 같은 기준으로 보고 같은 언어로 토론 가능
- **자신감 증가**: 이해관계자(팀/리드/고객)에게 “왜 이 모델인지” 설명이 쉬워짐

### 결과적으로 따라오는 실용 가치
- **시간 절약**: 비교표/차트 제작, 데이터 정리 반복 시간 감소
- **비용 절감**: 잘못된 모델 선택(재학습/재실험/재개발) 리스크 감소
- **품질 향상**: 회귀(regression) 탐지, 지표 간 Trade-off 가시화로 품질 관리 강화

---

## 5. 이 서비스는 누구에게 가장 먼저 필요할까요?

### 꼭 필요한 핵심 타깃 1순위는 누구인가요?
**“여러 후보 모델 중 1개를 빠르게 선정해야 하는 비전/ML 리드(또는 팀) + 실험이 잦은 연구/개발 조직”**

#### 구체적인 ‘절박한’ 초기 타깃 예시
- 주 1회 이상 모델 후보가 바뀌고, 매번 비교 근거를 문서로 만들어야 하는 팀
- 품질(PSNR/SSIM/LPIPS/FID)과 속도(FPS/latency) 사이에서 제품 의사결정을 반복하는 팀
- 평가를 여러 사람이 수행해 결과 정합성/설명 비용이 큰 팀

### 이 사람이 자발적으로 돈을 낼 이유는 무엇인가요?
- **돈으로 시간을 사는 구조**: 비교/정리/설명에 쓰는 시간을 줄이면 인건비가 바로 절감
- **의사결정 실패 비용 회피**: 잘못된 모델 선택은 제품 지연/품질 이슈로 큰 비용이 됨
- **팀 생산성 표준화**: 온보딩/협업/감사 로그 등 ‘운영 비용’을 줄여줌

> 초기 사업은 “모두를 위한 서비스”가 아니라 **“누군가에게 절박한 서비스”**여야 합니다.

---

## 6. 비용은 얼마가 적당한가요?

### 현재 지불하고 있는 비용은 얼마인가요?
- **명시적 비용**: 대부분 0원(스프레드시트/노트북)처럼 보임
- **숨은 비용(인건비)**:
  - 모델 비교/근거 자료 준비에 주당 2~6시간 × 팀원 수
  - 의사결정 지연/재실험/문서화 반복 비용

### 대체 서비스 대비 어느 정도인가요?
- 일반적인 MLOps/Experiment Tracking(예: Weights & Biases 등)은 강력하지만
  - **Vision 지표 중심의 “모델 선택/비교 UX”**는 팀에 맞게 커스텀해야 하는 경우가 많음
  - 가격도 개인/팀/엔터프라이즈로 올라가면 부담이 될 수 있음

### 월 구독 / 건별 / 기업 단가 중 어떤 구조인가요?
아래는 **속도(Speed) / 비용(Cost) / 품질(Quality)** 트레이드오프 관점의 옵션입니다.

#### 옵션 A: 월 구독(추천: 초기)
- **개인/소규모 팀**: 월 $19~$49
- **팀(5~20명)**: 월 $99~$299
- **적합한 이유**: “매주 반복되는 비교/정리/설명 시간을 줄여주는 도구” 성격과 잘 맞음

#### 옵션 B: 기업 단가(운영/보안 포함)
- **엔터프라이즈**: 월 $1,000~$5,000+
- 포함 예시: SSO, RBAC, 감사 로그, SLA, 온프레미스/전용 VPC 등
- **적합한 이유**: “운영/보안/표준화” 비용 절감 니즈가 큰 조직에 적합

#### 옵션 C: 건별 과금(평가 실행이 서버에서 일어날 때)
- 평가 실행이 실제 리소스를 소모한다면(예: GPU)
  - **평가 1회당** $0.5~$10+ (리소스/데이터 크기에 따라)
- **현재 프로젝트(로컬 중심)**에서는 우선순위가 낮고, 백엔드 연동 시 고려

---

## 쉬운 설명

Vision Arena가 해결하려는 핵심은 “차트를 예쁘게 그리는 것”이 아니라,  
**모델 선택이라는 반복되는 의사결정을 빠르고 확신 있게 만들고, 그 과정의 스트레스를 줄이는 것**입니다.

---

## 다음 단계 제안

1. **(가장 중요)** 초기 타깃을 1개로 고정  
   - 예) “제품 적용을 위해 Vision 모델을 고르는 ML 엔지니어 팀(주 1회 이상 비교)”
2. **측정 가능한 성공 지표 정의**
   - 예) 비교/리포트 작성 시간 50% 감소, 의사결정 리드타임 30% 감소
3. **(유료화의 핵심)** ‘반복’을 줄이는 워크플로우 기능 우선
   - 예) 템플릿 리포트 export, 기준선(baseline) 고정 비교, 승인/변경 이력
4. (향후) 백엔드/DB 연동 로드맵 확정
   - 역할/권한, 데이터 공유, 평가 파이프라인 자동 수집까지 확장


